{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a8144-6350-4b8d-b383-7c77664ebc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import importlib\n",
    "for module in ['kooplearn', 'matplotlib']:\n",
    "    try:\n",
    "        importlib.import_module(module)\n",
    "    except ImportError:\n",
    "        %pip install {module}\n",
    "import kooplearn\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from kooplearn.models import NystroemKernel\n",
    "from kooplearn.data import traj_to_contexts\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy \n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf802be-87bb-4ad2-bc06-13963e9b8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_windows_week(date_prediction):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------------------------------------------------------------\n",
    "\n",
    "    date_prediction : date with hour,\n",
    "    the date on which we want to predict the electricity consumption\n",
    "    ----------------------------------------------------------------\n",
    "\n",
    "    returns\n",
    "\n",
    "    windows : a dataframe which rows correspond to the dates on which a classifier will train\n",
    "\n",
    "    \"\"\"\n",
    "    date_prediction = pd.to_datetime(date_prediction)\n",
    "    last_training_date = date_prediction - pd.Timedelta(days=1)\n",
    "    first_training_date = date_prediction - pd.Timedelta(days=8)\n",
    "    concerned_hours = [3,8,13,18,23]\n",
    "\n",
    "    whole_training_dates = pd.date_range(start=first_training_date,end=last_training_date,freq='h')\n",
    "    training_dates = whole_training_dates[whole_training_dates.hour.isin(concerned_hours)].copy()\n",
    "    dict_windows = {}\n",
    "    \n",
    "    for i in range(len(concerned_hours)): #because we only want to predict the following day\n",
    "        begin_train_seq = first_training_date+pd.Timedelta(days=i)\n",
    "        end_train_seq = begin_train_seq + pd.Timedelta(days=31)\n",
    "        set_i = pd.date_range(start=begin_train_seq,end=end_train_seq,freq='h')\n",
    "        set_i = set_i[set_i.hour.isin(concerned_hours)]\n",
    "        dict_windows[f'set_{i}'] = set_i\n",
    "\n",
    "    windows = pd.DataFrame.from_dict(data=dict_windows)\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d01c36-3b51-4fed-ad5d-72466de11b8b",
   "metadata": {},
   "source": [
    "We first recover the data, sadly, there is no URL to use the different dataset, so I had to download them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24708e-625d-4e7e-822a-dd54dbb459d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_path = os.getcwd()\n",
    "os.chdir('/home/onyxia/work/Dynamical_system/data')\n",
    "data_2019 = pd.read_csv('data_2019.csv',sep=';')\n",
    "data_2020 = pd.read_csv('data_2020.csv',sep=';')\n",
    "data_2021 = pd.read_csv('data_2021.csv',sep=';')\n",
    "data_2022 = pd.read_csv('data_2022.csv',sep=';')\n",
    "prices = pd.read_csv('France.csv')\n",
    "os.chdir(actual_path)\n",
    "%run functions.ipynb\n",
    "%run mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c00119-7a0b-4fa9-8ff1-2b14df462a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = pd.concat([data_2019,data_2020,data_2021,data_2022])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac7deb-eb69-4a1f-a76d-9bb9cdd5cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = whole_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456e266-da0c-4260-a5c1-66c77d445467",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_drop = ['Périmètre','Prévision J-1','Prévision J',\n",
    "                ' Stockage batterie',\n",
    "                'Déstockage batterie','Eolien terrestre',\n",
    "                'Eolien offshore',\n",
    "                'Unnamed: 40','Nature']\n",
    "#the variables related to the batteries and both the ones \n",
    "#containing informations about offshore and onshore\n",
    "#wind are deleted because they only contain NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32807e-7efe-4e66-bc3b-8269d1cb371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data = whole_data.drop(list_to_drop,axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01126bd-0173-4a81-884e-fabbaa61cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data = use_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef201933-b981-4f04-a552-217c54daa712",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data.loc[:, 'Heures'] = use_data['Heures'].apply(lambda x: f\"{x}:00\" if len(x.split(':')) == 2 else x)\n",
    "use_data['Heures'] = pd.to_timedelta(use_data['Heures']) #a quick manipulation to add the hours to the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5eaef0-5c4b-4884-8a8b-3bc69d820303",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data['date'] = pd.to_datetime(use_data['Date']) + use_data['Heures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca6a75-8627-4f71-8893-380527b87bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data.drop(['Heures'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f878f63-113e-491e-b081-55bee3eab75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_period = prices[prices['Datetime (Local)'] >= '2019-01-01 00:00:00'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dcce8-e13c-4f2f-a9c7-adea4354ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_period #an overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37821e-6abf-410d-bce3-800a65d07274",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_period.drop(['Country','ISO3 Code','Datetime (UTC)'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506668c6-5be4-4846-98f5-aa3f15d752f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_period['Datetime (Local)'] = pd.to_datetime(prices_period['Datetime (Local)'])\n",
    "whole_period = prices_period.merge(use_data,how='left',left_on='Datetime (Local)',\n",
    "                                  right_on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968e344-69f3-4fa1-adf2-18d1b67df265",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period.drop('Datetime (Local)',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bc7bf-d6d9-4102-a279-bf6e62716ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d23912-ca25-452a-9619-896331d80950",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period.columns = ['price','date_wo_h','conso','fioul','coal','gas',\n",
    "                       'nuclear','wind','sun','hydro','pump','bioenergy','physics',\n",
    "                        'exchange_uk','exchange_sp','exchange_it','exchange_sw',\n",
    "                        'exchange_gr','co2_rate','fioul_tac','fioul_cogen',\n",
    "                        'fioul_other','gas_tac',\n",
    "                       'gas_cogen','gas_ccg','gas_other','hydro_river','hydro_lake',\n",
    "                       'hydro_turbine','bio_waste','bio_biomass','bio_biogas','date']\n",
    "#we rename the columns for clarity and simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3afd67-665e-4f9c-b10c-1fcf490c8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bedb4-edfe-4fdd-aaf3-2f18fcc5c88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(whole_period['date'],whole_period['price'])\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.title('the evolution of the price (euro/MWhe) of electricity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc178037-01af-458f-babf-af394fa7862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period['cos_day'] = whole_period['date'].dt.day.astype(float)\n",
    "whole_period['sin_day'] = whole_period['date'].dt.day.astype(float)\n",
    "whole_period['cos_month'] = whole_period['date'].dt.month.astype(float)\n",
    "whole_period['sin_month'] = whole_period['date'].dt.month.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41043b-9b07-49d9-b485-71663cfa3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period['cos_day'] = cos_transformer(365).fit_transform(whole_period['cos_day'])\n",
    "whole_period['cos_month'] = cos_transformer(12).fit_transform(whole_period['cos_month'])\n",
    "whole_period['sin_day'] = sin_transformer(365).fit_transform(whole_period['sin_day'])\n",
    "whole_period['sin_month'] = sin_transformer(12).fit_transform(whole_period['sin_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4e0ba-b313-4f36-b9f6-ec1bb0473915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots()\n",
    "for i in ['cos_day','sin_day','cos_month','sin_month']:\n",
    "    ax.plot(whole_period[f'{i}'],label=f'{i}')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a208230-5d25-45d5-be5c-a190d1810784",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period['weekdays'] = whole_period['date'].dt.dayofweek\n",
    "whole_period['weekend'] = np.zeros(whole_period.shape[0])\n",
    "whole_period['not_weekend'] = np.zeros(whole_period.shape[0])\n",
    "whole_period['date_wo_h'] = pd.to_datetime(whole_period['date_wo_h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f732e6c-171d-45db-a8d7-720364723868",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(whole_period.shape[0]):\n",
    "    if whole_period.loc[i,'weekdays'] == 5 or whole_period.loc[i,'weekdays'] == 6:\n",
    "        whole_period.loc[i,'weekend'] = 1\n",
    "    else:\n",
    "        whole_period.loc[i,'not_weekend'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e9628-5b8c-4a7a-bbd7-4a455adedb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add the 2 days lagged of fossil fuel and nuclear production\n",
    "#we cannot add what they call the announced availability because they don't \n",
    "#describe how they find it and on the majority of ressources concerning \n",
    "#the matter, we only have access to the effective production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51a5cc-f820-4bd4-9d3b-5662bd324a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period['2_lags_coal'] = whole_period['coal'].shift(48)\n",
    "whole_period['2_lags_fioul'] = whole_period['fioul'].shift(48)\n",
    "whole_period['2_lags_gas'] = whole_period['gas'].shift(48)\n",
    "whole_period['2_lags_nuke'] = whole_period['nuclear'].shift(48)\n",
    "whole_period['target_price'] = whole_period['price'].shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ef5da-9164-4ff8-aa4b-d34313626424",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_period.dropna(inplace=True) #we just loose two days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cac5b-3418-4a85-b68d-ed9014542691",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = whole_period[whole_period['date_wo_h'].dt.year < 2022].copy()\n",
    "test_data = whole_period[whole_period['date_wo_h'].dt.year == 2022].copy()\n",
    "list_imp_hours = [3,8,13,18,23] #as done in the article, we select those hours as it reduces the computational \n",
    "#cost and it represents well the evolution of the data over the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59749cf1-6d5a-473e-a34c-331ce5e8cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data[training_data['date'].dt.hour.isin(list_imp_hours)]\n",
    "test_data = test_data[test_data['date'].dt.hour.isin(list_imp_hours)]\n",
    "for_plots_date = test_data['date']\n",
    "for_plots_train = training_data['date']\n",
    "training_data.reset_index(inplace=True,drop=True)\n",
    "test_data.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d35b1-2fed-4462-9164-e9337509a254",
   "metadata": {},
   "source": [
    "## A simple implementation on a single day\n",
    "We use the make_training_windows_week function that creates training windows corresponding to the procedure of the 'new' OSSCP horizon approach. An important part of the innovations provided in the article is the use of an online aggregation procedure made possible by the new 'splitting' of the data. \n",
    "\n",
    "Concerning the feasibility of this experiment, it became hardly reproductible in Python as the package they use (OPERA) is in R. After a bit of research, it appears that there was a version of this package in Python but after a few hours of test (as the documentation is quite short and some properties are not explained in great detail), I managed to run the main functions. However, it turns out that an error appears. And after having checked the code of the said function, the error came from the fact that if there was an 'if' missing depending on the options used to run it. As I am not very familiar with the topics evolving around online learning, I thought it would be better not to try and correct their code as I could spend too much time on it with poor results. \n",
    "\n",
    "On this first experiment, we will try to predict the hourly prices of the 29th of may of 2019. I chose this date because in theory, there shouldn't be too much volatility in the prices as less electricity is needed in summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fd524-0c23-456f-b107-dcc15d9abcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dates = pd.date_range(start='2019-01-03',end='2019-01-14',freq='h')\n",
    "training_dates = training_dates[training_dates.hour.isin(list_imp_hours)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d9da0-dcba-49e3-a04d-3067b869ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_windows = make_training_windows_week('2019-05-29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace7e20-6bf0-455f-be59-69e293fee89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_05_dict = {}\n",
    "reg_95_dict = {}\n",
    "reg_50_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cdb88-2bed-4203-bf55-c1dd85326d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in get_windows.columns:\n",
    "    list_train = get_windows[i]\n",
    "    training_set_part = training_data[training_data['date'].isin(list_train)]\n",
    "    reg_05_dict[f'reg_{count}'] = QuantileRegressor(quantile=0.05).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                  y=training_set_part['target_price'])\n",
    "    reg_95_dict[f'reg_{count}'] = QuantileRegressor(quantile=0.95).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                  y=training_set_part['target_price'])\n",
    "    reg_50_dict[f'reg_{count}'] = QuantileRegressor(quantile=0.5).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                  y=training_set_part['target_price'])\n",
    "    count += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de38646-1117-4b3e-8303-c8161f321743",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_05 = np.zeros((0,5))\n",
    "predictions_95 = np.zeros((0,5))\n",
    "predictions_50 = np.zeros((0,5))\n",
    "for i in range(count-1):\n",
    "    to_pred = training_data[training_data['date_wo_h'] == pd.to_datetime('2019-05-29')].drop(['target_price','date','date_wo_h'],axis=1)\n",
    "    predictions_05 = np.vstack((predictions_05,reg_05_dict[f'reg_{i}'].predict(to_pred)))\n",
    "    predictions_95 = np.vstack((predictions_95,reg_95_dict[f'reg_{i}'].predict(to_pred)))\n",
    "    predictions_50 = np.vstack((predictions_50,reg_50_dict[f'reg_{i}'].predict(to_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff09ab-ba9c-4121-8044-781f7a8b587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(training_data[training_data['date_wo_h'] == pd.to_datetime('2019-05-29')]['target_price'])\n",
    "fig, ax = plt.subplots()\n",
    "index = np.linspace(start=1,stop=5,num=5)\n",
    "for i in range(3):\n",
    "    ax.plot(index,predictions_05[i,:],'r--')\n",
    "    ax.plot(index,predictions_95[i,:],'b--')\n",
    "    ax.plot(index,predictions_50[i,:],'k--')\n",
    "    ax.fill_between(index,predictions_05[i,:],predictions_95[i,:],color='salmon',alpha=0.3)\n",
    "\n",
    "ax.plot(index,predictions_05[3,:],'r--',label='0.05 quantile')\n",
    "ax.plot(index,predictions_95[3,:],'b--',label='0.95 quantile')\n",
    "ax.plot(index,predictions_50[3,:],'k--',label='0.5 quantile')\n",
    "ax.fill_between(index,predictions_05[3,:],predictions_95[3,:],color='salmon',alpha=0.3,label='0.9 CI')\n",
    "plt.plot(index,test,'g-',label='true price')\n",
    "plt.title('predictions of quantile regression on a single day')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063eb8ef-beb9-48c8-b22c-7423316afd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after a few tries demanding a lot of computational power\n",
    "#I selected a few variables that seemed, as said in the article\n",
    "#the most important ones\n",
    "copy_koop = training_data[['price','target_price','nuclear','2_lags_nuke']].copy()\n",
    "for col in copy_koop.columns:\n",
    "    copy_koop[col] = stats.zscore(copy_koop[col]) \n",
    "copy_koop['target_last'] = copy_koop['target_price'].copy()\n",
    "copy_koop.drop('target_price',axis=1,inplace=True)\n",
    "copy_koop_np = copy_koop.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799a2b3-bd8f-47af-a8b9-aa3a840f0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RBF(length_scale=0.8)\n",
    "tr_ctx = traj_to_contexts(copy_koop_np[:30,:],time_lag=2,context_window_len=3)\n",
    "ts_ctx = traj_to_contexts(copy_koop_np[30:40,:],time_lag=2,context_window_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f654755-3c8e-4cf1-8574-3207e7a19f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nys_rrr = NystroemKernel(kernel=kernel,reduced_rank=True,tikhonov_reg=1e-8,rank=10,num_centers=500)\n",
    "nys_rrr.fit(tr_ctx)\n",
    "pred = nys_rrr.predict(ts_ctx)\n",
    "true = ts_ctx.lookforward(nys_rrr.lookback_len)\n",
    "rmse = np.sqrt((np.sum((pred-true)**2)))/ts_ctx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5a4a1-e3db-4585-b81b-4736f10c6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373cb7dd-3c82-4ff0-97c4-8bb0af854f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.grid(alpha=0.3)\n",
    "ax.plot(pred[:,0,-1],'-',color='royalblue',label='prediction')\n",
    "ax.plot(copy_koop_np[34:40,-1],'g-',label='actual price')\n",
    "ax.legend()\n",
    "plt.title('predictions using kooplearn (prices are standardised)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ba957-f57c-4108-9d03-20c7ffeae0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bis = nys_rrr.predict(tr_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb27da0-4efb-4b84-8a99-bcb365eb9d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.plot(pred_bis[:,0,-1],'b--',label='prediction')\n",
    "ax.plot(copy_koop_np[4:30,-1],'g-',label='actual price')\n",
    "ax.legend()\n",
    "plt.title('predictions on the training data (prices are standardised)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99cbfa-8aa0-460d-ad65-69bfda07f34b",
   "metadata": {},
   "source": [
    "## On a whole month \n",
    "As precised in the article, before 2020, the different methods managed to have very satisfying results. The prices were relatively smooth and hence good performance could be reached. In order to compare it with the difficult periods following 2020 and even 2023 (which seems to be an even more volatile year), we first try on a month that once again seems to be pretty 'simple'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d792e1-3c34-4515-927a-1255c3dbcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_june = pd.date_range(start='2019-06-01',end='2019-06-30',freq='h')\n",
    "target_june_dates = pd.date_range(start='2019-06-01',end='2019-06-30',freq='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0a580-99fc-46c9-b3cb-721e1efb247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_05_june = {}\n",
    "reg_50_june = {}\n",
    "reg_95_june = {}\n",
    "pred_whole_05 = np.zeros((4,0))\n",
    "pred_whole_95 = np.zeros((4,0))\n",
    "pred_whole_50 = np.zeros((4,0))\n",
    "predictions_koop = np.zeros((0,4))\n",
    "for day in tqdm(target_june_dates):\n",
    "    predictions_05_june = np.zeros((0,5))\n",
    "    predictions_95_june = np.zeros((0,5))\n",
    "    predictions_50_june = np.zeros((0,5))\n",
    "    corresponding_training_windows = make_training_windows_week(day)\n",
    "    count = 0\n",
    "    for col in corresponding_training_windows.columns:\n",
    "        list_train = corresponding_training_windows[col]\n",
    "        training_set_part = training_data[training_data['date'].isin(list_train)]\n",
    "        reg_05_june[f'reg_{count}'] = QuantileRegressor(quantile=0.05).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        reg_95_june[f'reg_{count}'] = QuantileRegressor(quantile=0.95).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        reg_50_june[f'reg_{count}'] = QuantileRegressor(quantile=0.5).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        count += 1\n",
    "    for i in range(count-1):\n",
    "        to_pred = training_data[training_data['date_wo_h'] == pd.to_datetime(day)].drop(['target_price','date','date_wo_h'],axis=1)\n",
    "        predictions_05_june = np.vstack((predictions_05_june,reg_05_june[f'reg_{i}'].predict(to_pred)))\n",
    "        predictions_95_june = np.vstack((predictions_95_june,reg_95_june[f'reg_{i}'].predict(to_pred)))\n",
    "        predictions_50_june = np.vstack((predictions_50_june,reg_50_june[f'reg_{i}'].predict(to_pred)))\n",
    "    pred_whole_05 = np.hstack((pred_whole_05,predictions_05_june))\n",
    "    pred_whole_50 = np.hstack((pred_whole_50,predictions_50_june))\n",
    "    pred_whole_95 = np.hstack((pred_whole_95,predictions_95_june))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2c8c7-695f-4cae-a992-7917423619de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_june = training_data[training_data['date_wo_h'].isin(target_june_dates)]['target_price']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index_june = np.linspace(start=1,stop=150,num=150)\n",
    "ax.grid(alpha=0.3)\n",
    "for i in range(4):\n",
    "    ax.plot(index_june,pred_whole_05[i,:],'r--')\n",
    "    ax.plot(index_june,pred_whole_95[i,:],'b--')\n",
    "    ax.plot(index_june,pred_whole_50[i,:],'k--')\n",
    "    ax.fill_between(index_june,pred_whole_05[i,:],pred_whole_95[i,:],color='salmon',alpha=0.3)\n",
    "\n",
    "ax.plot(index_june,pred_whole_05[3,:],'r--',label='0.5 quantile')\n",
    "ax.plot(index_june,pred_whole_95[3,:],'b--',label='0.95 quantile')\n",
    "ax.plot(index_june,pred_whole_50[3,:],'k--',label='median')\n",
    "ax.fill_between(index_june,pred_whole_05[3,:],pred_whole_95[i,:],color='salmon',alpha=0.3,label='0.9 CI')\n",
    "\n",
    "plt.plot(index_june,test_june,'g-',label='true price')\n",
    "plt.title('predictions without postprocessing (with all the estimators)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dcc65-30fb-4431-801f-38d5b4c3eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_up = np.mean(pred_whole_95,axis=0)\n",
    "mean_down = np.mean(pred_whole_05,axis=0)\n",
    "mean_mid = np.mean(pred_whole_50,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d5a64-1ca3-4042-ab4b-159e582c0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.plot(index_june,mean_up,'b-',label='mean 0.95 quantiles')\n",
    "ax.plot(index_june,mean_down,'r-',label='mean 0.05 quantiles')\n",
    "ax.plot(index_june,mean_mid,'k-',label='mean 0.5 quantiles')\n",
    "ax.fill_between(index_june,mean_up,mean_down,color='salmon',alpha=0.3)\n",
    "plt.plot(index_june,test_june,'g-',label='true price')\n",
    "plt.title('forecasts with agregation (mean)')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89cbdb-4949-4c86-94fb-47c1a0215e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff = np.mean(mean_up-mean_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2632c-d9cf-41e0-a55b-997f525dee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9473a5-6707-46c2-9482-2ddae479aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(150):\n",
    "    if np.array(test_june)[i]>mean_down[i] and np.array(test_june)[i]<mean_up[i]:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412af19-c636-433c-b481-8bab3aec6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c274266-5e6d-404d-b5d3-e40343947652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we do not exactly reach the 90% coverage desired, that is why there is postprocessing\n",
    "#combining both online agregation and the calibration part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa1ee4-b6ed-456f-b707-d4b40259f0fb",
   "metadata": {},
   "source": [
    "As a conclusion on the results of the quantile regression, we can say that we don't reach the 90% of coverage that was supposed to be reached. However, we can explain this by different reasons. First, I didn't perform a grid search to find the best parameters to get the best results. Moreover, this is supposed to be a sort of 'benchmark value' to see to what extent we could improve this model.\n",
    "\n",
    "Let's now implement the basics of what OSSCP looks like. As I wasn't familiar with the whole procedures, I implemented the calibration proposed in the beginning of the article, using the quantiles of the residuals of the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628ca0a-eb9b-408d-9de0-315359e18a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_05_june = {}\n",
    "reg_50_june = {}\n",
    "reg_95_june = {}\n",
    "pred_whole_05 = np.zeros((4,0))\n",
    "pred_whole_95 = np.zeros((4,0))\n",
    "pred_whole_50 = np.zeros((4,0))\n",
    "predictions_koop = np.zeros((0,4))\n",
    "calibration_results = np.zeros((0,5))\n",
    "for day in tqdm(target_june_dates):\n",
    "    predictions_05_june = np.zeros((0,5))\n",
    "    predictions_95_june = np.zeros((0,5))\n",
    "    predictions_50_june = np.zeros((0,5))\n",
    "    corresponding_training_windows = make_training_windows_week(day)\n",
    "    count = 0\n",
    "    for col in corresponding_training_windows.columns:\n",
    "        list_train = corresponding_training_windows[col]\n",
    "        training_set_part = training_data[training_data['date'].isin(list_train)]\n",
    "        reg_05_june[f'reg_{count}'] = QuantileRegressor(quantile=0.05).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        reg_95_june[f'reg_{count}'] = QuantileRegressor(quantile=0.95).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        reg_50_june[f'reg_{count}'] = QuantileRegressor(quantile=0.5).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        count += 1\n",
    "    for i in range(count-1):\n",
    "        to_pred = training_data[training_data['date_wo_h'] == pd.to_datetime(day)].drop(['target_price','date','date_wo_h'],axis=1)\n",
    "        predictions_05_june = np.vstack((predictions_05_june,reg_05_june[f'reg_{i}'].predict(to_pred)))\n",
    "        predictions_95_june = np.vstack((predictions_95_june,reg_95_june[f'reg_{i}'].predict(to_pred)))\n",
    "        predictions_50_june = np.vstack((predictions_50_june,reg_50_june[f'reg_{i}'].predict(to_pred)))\n",
    "        true_values = np.array(training_data[training_data['date_wo_h'] == pd.to_datetime(day)]['target_price'])\n",
    "        residuals = np.abs(true_values-reg_50_june[f'reg_{i}'].predict(to_pred)) #we compute the residuals\n",
    "        calibration_results = np.vstack((calibration_results,residuals))\n",
    "    flattened_cal = np.ravel(calibration_results)\n",
    "    value_to_as = np.quantile(a=flattened_cal,q=0.08) #we find the 'deflated' quantile of the residuals, \n",
    "    #as it is presented in the article\n",
    "    predictions_05_june -= value_to_as #we add and substract the desired quantile \n",
    "    #in order to obtain an effective coverage of 0.9\n",
    "    predictions_95_june += value_to_as\n",
    "    pred_whole_05 = np.hstack((pred_whole_05,predictions_05_june))\n",
    "    pred_whole_50 = np.hstack((pred_whole_50,predictions_50_june))\n",
    "    pred_whole_95 = np.hstack((pred_whole_95,predictions_95_june))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623265d-2179-4684-9733-c20b29229783",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_up = np.mean(pred_whole_95,axis=0)\n",
    "mean_down = np.mean(pred_whole_05,axis=0)\n",
    "mean_mid = np.mean(pred_whole_50,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8428b4-8f00-4ed0-9a5d-54835f88f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.plot(index_june,mean_up,'b-',label='mean 0.95 quantiles')\n",
    "ax.plot(index_june,mean_down,'r-',label='mean 0.05 quantiles')\n",
    "ax.plot(index_june,mean_mid,'k-',label='mean 0.5 quantiles')\n",
    "ax.fill_between(index_june,mean_up,mean_down,color='salmon',alpha=0.3)\n",
    "plt.plot(index_june,test_june,'g-',label='true price')\n",
    "plt.title(\"forecasts with agregation (mean) and a bit of OSSCP\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985cb78-5b79-4f51-9ee4-83f099dc8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff = np.mean(mean_up-mean_down)\n",
    "print('the average length of the theoretical 0.9 CI is : ',avg_diff)\n",
    "correct = 0\n",
    "for i in range(150):\n",
    "    if np.array(test_june)[i]>mean_down[i] and np.array(test_june)[i]<mean_up[i]:\n",
    "        correct += 1\n",
    "print(correct/150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7183a3b-0689-4719-882a-6dfe853ff2f5",
   "metadata": {},
   "source": [
    "We can notice that with almost no difference in terms of the length of the CI, we get a much better coverage.\n",
    "As I couldn't implement the correct and optimal online aggregation, it seems of course very promising and the width of the CI would probably decrease a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c03d0-a88f-48e2-b2c3-4366ab12ee3f",
   "metadata": {},
   "source": [
    "## Let's try the Koopmans operator approach on the same period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8b0aa-137c-4b2b-bfbc-9d2713ab4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RBF(length_scale=0.8)\n",
    "nys_rrr_june = NystroemKernel(kernel=kernel,reduced_rank=True,tikhonov_reg=1e-8,rank=10,num_centers=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee581c03-e40e-4f86-b357-ae3175d84d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nys_rrr.fit(tr_ctx)\n",
    "pred = nys_rrr.predict(ts_ctx)\n",
    "true = ts_ctx.lookforward(nys_rrr.lookback_len)\n",
    "rmse = np.sqrt((np.sum((pred-true)**2)))/ts_ctx.shape[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfacfce-e450-47bd-861f-257aceb3cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "june_and_before = pd.date_range(start='2019-05-21',end='2019-07-01',freq='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede6f45-c026-4769-a1cb-3276cd44adfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "copy_koop_june = training_data[training_data['date_wo_h'].isin(june_and_before)].copy()\n",
    "copy_koop_june = copy_koop_june[['date_wo_h','date','price','target_price','nuclear','gas','coal']]\n",
    "for col in copy_koop_june.drop(['date_wo_h','date'],axis=1).columns:\n",
    "    copy_koop_june[col] = stats.zscore(copy_koop_june[col])\n",
    "copy_koop_june['target_last'] = copy_koop_june['target_price'].copy()\n",
    "copy_koop_june.drop('target_price',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc78d7d-3102-4176-8e58-c88f0c956bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f43ad-f493-4c44-8ba3-30a7edc0307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_pred = np.zeros((0,5))\n",
    "true_data_whole = np.zeros((0,5))\n",
    "nys_rrr_june = NystroemKernel(kernel=kernel,reduced_rank=True,tikhonov_reg=1e-8,rank=10,num_centers=500)\n",
    "for day in tqdm(target_june_dates):\n",
    "    start_train = day - pd.Timedelta(7,\"d\")\n",
    "    train_period = pd.date_range(start=start_train,end=day + pd.Timedelta(1,'d'),freq='d')\n",
    "    temp_train = copy_koop_june[copy_koop_june['date_wo_h'].isin(train_period)].drop(['date','date_wo_h'],axis=1)\n",
    "    koop_np_temp = np.array(temp_train)\n",
    "    tr_ctx = traj_to_contexts(koop_np_temp[:-10,:],time_lag=2,context_window_len=3)\n",
    "    ts_ctx = traj_to_contexts(koop_np_temp[-10:,:],time_lag=2,context_window_len=3)\n",
    "    nys_rrr_june.fit(tr_ctx)\n",
    "    pred = nys_rrr_june.predict(ts_ctx)\n",
    "    pred_to_stack = np.squeeze(pred)\n",
    "    whole_pred = np.vstack((whole_pred,pred_to_stack))\n",
    "    true_data = np.squeeze(ts_ctx.lookforward(nys_rrr_june.lookback_len))\n",
    "    true_data_whole = np.vstack((true_data_whole,true_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a2f3c-952a-4d2d-b1aa-3cbd35816617",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(whole_pred[:,-1])\n",
    "plt.plot(true_data_whole[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd6ab4-1afb-4f40-901e-e79962a81583",
   "metadata": {},
   "source": [
    "We can notice that the predictions are quite far from the actual trajectory. This is the last plot obtained after having tried with different variables and different values of the NystroemKernel class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f507e-995b-4e64-b8b5-a50b833398f3",
   "metadata": {},
   "source": [
    "### Now, let's try to implement OSSCP-horizon on a month with high volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e550f60-2837-4415-8bb9-f726015b4caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c53c1ed-8171-41bd-a97c-7c4c3fe8e97a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yesy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43myesy\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yesy' is not defined"
     ]
    }
   ],
   "source": [
    "yesy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722c917-327d-4790-ae0f-88233a013b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_05_sept = {}\n",
    "reg_50_sept = {}\n",
    "reg_95_sept = {}\n",
    "pred_whole_05 = np.zeros((4,0))\n",
    "pred_whole_95 = np.zeros((4,0))\n",
    "pred_whole_50 = np.zeros((4,0))\n",
    "predictions_koop = np.zeros((0,4))\n",
    "calibration_results = np.zeros((0,5))\n",
    "for day in tqdm(target_sept_dates):\n",
    "    predictions_05_sept = np.zeros((0,5))\n",
    "    predictions_95_sept = np.zeros((0,5))\n",
    "    predictions_50_sept = np.zeros((0,5))\n",
    "    corresponding_training_windows = make_training_windows_week(day)\n",
    "    count = 0\n",
    "    for col in corresponding_training_windows.columns:\n",
    "        list_train = corresponding_training_windows[col]\n",
    "        training_set_part = training_data[training_data['date'].isin(list_train)]\n",
    "        reg_05_sept[f'reg_{count}'] = QuantileRegressor(quantile=0.05).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        reg_95_sept[f'reg_{count}'] = QuantileRegressor(quantile=0.95).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        reg_50_sept[f'reg_{count}'] = QuantileRegressor(quantile=0.5).fit(X=training_set_part.drop(['target_price','date','date_wo_h'],axis=1),\n",
    "                                                                      y=training_set_part['target_price'])\n",
    "        count += 1\n",
    "    for i in range(count-1):\n",
    "        to_pred = training_data[training_data['date_wo_h'] == pd.to_datetime(day)].drop(['target_price','date','date_wo_h'],axis=1)\n",
    "        predictions_05_sept = np.vstack((predictions_05_sept,reg_05_sept[f'reg_{i}'].predict(to_pred)))\n",
    "        predictions_95_sept = np.vstack((predictions_95_sept,reg_95_sept[f'reg_{i}'].predict(to_pred)))\n",
    "        predictions_50_sept = np.vstack((predictions_50_sept,reg_50_sept[f'reg_{i}'].predict(to_pred)))\n",
    "        true_values = np.array(training_data[training_data['date_wo_h'] == pd.to_datetime(day)]['target_price'])\n",
    "        residuals = np.abs(true_values-reg_50_sept[f'reg_{i}'].predict(to_pred)) #we compute the residuals\n",
    "        calibration_results = np.vstack((calibration_results,residuals))\n",
    "    flattened_cal = np.ravel(calibration_results)\n",
    "    value_to_as = np.quantile(a=flattened_cal,q=0.08) #we find the 'deflated' quantile of the residuals, \n",
    "    #as it is presented in the article\n",
    "    predictions_05_sept -= value_to_as #we add and substract the desired quantile \n",
    "    #in order to obtain an effective coverage of 0.9\n",
    "    predictions_95_sept += value_to_as\n",
    "    pred_whole_05 = np.hstack((pred_whole_05,predictions_05_sept))\n",
    "    pred_whole_50 = np.hstack((pred_whole_50,predictions_50_sept))\n",
    "    pred_whole_95 = np.hstack((pred_whole_95,predictions_95_sept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb3a79-483d-484f-a238-097ac1c67743",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_up = np.mean(pred_whole_95,axis=0)\n",
    "mean_down = np.mean(pred_whole_05,axis=0)\n",
    "mean_mid = np.mean(pred_whole_50,axis=0)\n",
    "index_sept = np.linspace(start=1,stop=150,num=150)\n",
    "test_sept = training_data[training_data['date_wo_h'].isin(target_sept_dates)]['target_price']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.plot(index_sept,mean_up,'b-',label='mean 0.95 quantiles')\n",
    "ax.plot(index_sept,mean_down,'r-',label='mean 0.05 quantiles')\n",
    "ax.plot(index_sept,mean_mid,'k-',label='mean 0.5 quantiles')\n",
    "ax.fill_between(index_sept,mean_up,mean_down,color='salmon',alpha=0.3)\n",
    "plt.plot(index_sept,test_sept,'g-',label='true price')\n",
    "plt.title(\"forecasts with agregation (mean) and a bit of OSSCP\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903a58b-f705-421e-a953-33deed77d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_up = np.mean(pred_whole_95,axis=0)\n",
    "mean_down = np.mean(pred_whole_05,axis=0)\n",
    "mean_mid = np.mean(pred_whole_50,axis=0)\n",
    "avg_diff = np.mean(mean_up-mean_down)\n",
    "print('the average width is : ',avg_diff)\n",
    "correct = 0\n",
    "for i in range(150):\n",
    "    if np.array(test_sept)[i]>mean_down[i] and np.array(test_sept)[i]<mean_up[i]:\n",
    "        correct += 1\n",
    "print('the empirical coverage is : ',correct/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d93f21-16cb-4a5e-a211-02c661b7720f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
